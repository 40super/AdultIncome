07/25/24: 
Gonna take some notes on the process I have been doing..

In addition to those possible questions, I think doing an analysis per race can help understand trends within race and then go to a overall answer to general facts based on that.

In terms of labeling, I think I will add a year of data at the end for columns to separate.

I started importing the first set of data (epi_cpsbasic_2020.dta) data to a notebook where I then started listing any significant columns that might help with my analysis and possibly in creating a machine learning prediction model but that is not important now. I started the cleansing part where I created a dataframe with a set of columns that had significance in the analysis most likely but as I import columns, I also check if there are any 'NaN' values. I started catching columns where there were about > 1 million entries that are NaN which hinders the analysis because some of these columns can help with understanding the bigger picture. So for now I will remove them if there are too many missing from the equation and look for other variables.

After some digging I ended up with this column list : ['region','statefips','age','female','wbhao','educ','faminc','emp','dlfstat','dind03','mocc10','occ18','multjobs','numjobs','metstat','cbsasize','citistat','citizen','famrel','married','veteran','basicwgt']
census_2020 = epi_dta_2020_df[datacolumns]

I also found out with the pandas .read_stata function, I was able to keep the value as categorical so the output won't be numbers but rather the categories. I will have to look into this and see if there is a way to use this for machine learning but I will most likely have to create nominal data that represents the categories.

If I drop all NA and have a clean dataset, I will go from 1.2 mil to 550k. This is about 55% loss but with a sample size this big, I believe the loss won't be significant. In Addition, all the data is categorical so there are no numbers or variables that can be normalized instead of dropping. I did try to see if there were other columns I can import to lessen the NaN count but majority of the variables are needed.

After some digging, All of the NaN value were because of unemployment. If employed they would have data so we are gonna assume employment

I wrapped up and created a function that will process the epi data and clean it so it can be then moved to the next steps.

07/30/24:
The next steps that were performed was to add a year of data column so that when combined all together for the machine learning portion most likely or tableau analysis, the data can be distinguished of what year the data was posted. After that I did some more minor cleaning such as restting index. Aftter that a function was made to take in the stata data and the year of the data to process it and clean it. Once that was done, I did minor cleaning on some columns that had uneccessary information. Finally I concat the 2024 first 6 months data and the created a master table that had info from 2020-2024. With this I then export the data in both csv and excel format where the next step for the csv would be used for the machine learning portion if time is applicable. The excel data is gonna be used to so some more slight cleaning as well as data formatting to match whats best. Once that is done we will do a pivot table to find some information and then use tableu to visualize any insights on the question we had . 

When looking at the data there seem to be a certain skew of information so I believe creating a subset of the data to use for analysis will help with creating appropriate insight.



08/01/24:

So starting off, I wanted to create a sample from the master table due to certain attributes such as race where there were a majority of 'White' entries and little of 'Other'entries available. I want to create a subset for each set of race based on region as well so for example(not actual numbers) if Midwest region had 90k White, 25K Asian, 50k Black, 10K Other. I will then add a Random Column() that has random values to allow a unbias selection of data to make sure data wont be skewed. I would then take subset based on the smallest amount in this case there is 10k others , so I will take 5k of Other, White, Asian, and Black to allow for equal set of data to be used to make proper analysis. I did come across the issue where Due to the volume of data which is over 500k per year. Getting samples was difficult due to excel struggling with lag and as well being unresponsive. I think the next step is to use python to create sample which then will be brought back to Excel for some minor pivot table analysis then once getting a better understanding of the data I will take it to Tableau. I will also do some minor adjustment to the data such as  removing ages 80+ due to all other ages being an int so doing any mathematical arithmetic will not work due to 80+ being a string. 

08/06/24:

I began sampling the data through python. I first created a function that takes a dataframe followed by the sample size parameters needed to generate a random sample. I then split by race and sample a set amount which was 15000 per race and 6000 for other to have a solid sample to analyze later. The reason I sample is due to certain population such as the White race having 2-3 times more data then the other races which can cause some skewness in terms of results so I decided to randomly sample from each race but this will still provide valuable insight due to the number of samples is still significant enough to represent the population sample. I then export this data to a csv and an excel which contains all of the information.

The next step is to rename columns in the excel file to be used later for Tableau and as well as formatting our data information such as age now which can be represented as an int instead of string due to removing the 80+ statistic which I believe won't affect the overall statistical results which I am looking for.

08/08/24:

I began cleaning with excel and changing columns names to be more readable and as well as change datatypes such age from text to int. In addition, I binned ages as well for analytical purposes where ages less 43 was in one bin while ages greater than 42 in another bin. I the created multiple pivot tables to see if there was a correlation between age and income which after analyzing the data there was no correlation. I then create a simple visualization to show the distribution of the ages to show there is no bias.
Next step is to do more pivot table analysis before moving on to Tableau


08/13/24:
I did the next two analysis question which was income based on race and income based on education. I created more pivot tables as well as  some charts to visualize that. Gonna do a couple more and then move on to Tableau.

08/15/24:
I finished excel vis and made a master csv sample using python which then I imported to Tableau. Biggest issues since alot of the data is categorical and not continuous there is issues creating meaningful data so I will create basic graph highlighting certain information in the future so i can make multiple dashboards to explain each situation.