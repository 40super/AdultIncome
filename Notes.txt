07/25/24: 
Gonna take some notes on the process I have been doing..

In addition to those possible questions, I think doing an analysis per race can help understand trends within race and then go to a overall answer to general facts based on that.

In terms of labeling, I think I will add a year of data at the end for columns to separate.

I started importing the first set of data (epi_cpsbasic_2020.dta) data to a notebook where I then started listing any significant columns that might help with my analysis and possibly in creating a machine learning prediction model but that is not important now. I started the cleansing part where I created a dataframe with a set of columns that had significance in the analysis most likely but as I import columns, I also check if there are any 'NaN' values. I started catching columns where there were about > 1 million entries that are NaN which hinders the analysis because some of these columns can help with understanding the bigger picture. So for now I will remove them if there are too many missing from the equation and look for other variables.

After some digging I ended up with this column list : ['region','statefips','age','female','wbhao','educ','faminc','emp','dlfstat','dind03','mocc10','occ18','multjobs','numjobs','metstat','cbsasize','citistat','citizen','famrel','married','veteran','basicwgt']
census_2020 = epi_dta_2020_df[datacolumns]

I also found out with the pandas .read_stata function, I was able to keep the value as categorical so the output won't be numbers but rather the categories. I will have to look into this and see if there is a way to use this for machine learning but I will most likely have to create nominal data that represents the categories.

If I drop all NA and have a clean dataset, I will go from 1.2 mil to 550k. This is about 55% loss but with a sample size this big, I believe the loss won't be significant. In Addition, all the data is categorical so there are no numbers or variables that can be normalized instead of dropping. I did try to see if there were other columns I can import to lessen the NaN count but majority of the variables are needed.

After some digging, All of the NaN value were because of unemployment. If employed they would have data so we are gonna assume employment

I wrapped up and created a function that will process the epi data and clean it so it can be then moved to the next steps.

07/30/24:
The next steps that were performed was to add a year of data column so that when combined all together for the machine learning portion most likely or tableau analysis, the data can be distinguished of what year the data was posted. After that I did some more minor cleaning such as restting index. Aftter that a function was made to take in the stata data and the year of the data to process it and clean it. Once that was done, I did minor cleaning on some columns that had uneccessary information. Finally I concat the 2024 first 6 months data and the created a master table that had info from 2020-2024. With this I then export the data in both csv and excel format where the next step for the csv would be used for the machine learning portion if time is applicable. The excel data is gonna be used to so some more slight cleaning as well as data formatting to match whats best. Once that is done we will do a pivot table to find some information and then use tableu to visualize any insights on the question we had . 

When looking at the data there seem to be a certain skew of information so I believe creating a subset of the data to use for analysis will help with creating appropriate insight.



08/01/24:

So starting off, I wanted to create a sample from the master table due to certain attributes such as race where there were a majority of 'White' entries and little of 'Other'entries available. I want to create a subset for each set of race based on region as well so for example(not actual numbers) if Midwest region had 90k White, 25K Asian, 50k Black, 10K Other. I will then add a Random Column() that has random values to allow a unbias selection of data to make sure data wont be skewed. I would then take subset based on the smallest amount in this case there is 10k others , so I will take 5k of Other, White, Asian, and Black to allow for equal set of data to be used to make proper analysis. I did come across the issue where Due to the volume of data which is over 500k per year. Getting samples was difficult due to excel struggling with lag and as well being unresponsive. I think the next step is to use python to create sample which then will be brought back to Excel for some minor pivot table analysis then once getting a better understanding of the data I will take it to Tableau. I will also do some minor adjustment to the data such as  removing ages 80+ due to all other ages being an int so doing any mathematical arithmetic will not work due to 80+ being a string. 